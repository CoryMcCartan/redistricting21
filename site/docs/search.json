{
  "articles": [
    {
      "path": "about.html",
      "title": "About",
      "author": [],
      "contents": "\nThe 2021 redistricting cycle will be extremely consequential, setting legislative and congressional boundaries for a decade, just as computing power and new data allow map-drawers to create sophisticated partisan gerrymanders, and the Supreme Court has stepped back from policing them.\nWe put together this site to analyze redistricting plans as states release them, in context. In context means we will employ cutting-edge redistricting simulation methodologies to create large numbers of alternative plans. This helps make better judgement about the quality of a proposed or finalized plan, by making comparisons to other plans which also respect municipal, state, and federal requirements for redistricting.\nWe rely entirely on open source tools and data, so that anyone (with a bit of R familiarity) can replicate the analysis and make changes to it.\nWe will attempt to cover as many state plans as we are able. This map summarizes the state of our current analyses.\nOur analysesAbout the Authors\nCory McCartan is a PhD Candidate in Statistics at Harvard University. He is the author of the R packages blockpop, wacolors, and adjustr.\nChris Kenny is a PhD Candidate in Government at Harvard University. He studies redistricting and Census policy-making. He is the author of the R packages geomander and ppmf.\nThe authors are members of the Algorithm Assisted Redistricting Methodology (ALARM) Project, and are coauthors of the R packages redist (with Ben Fifield and Kosuke Imai) and PL94171.\n\n\n\n",
      "last_modified": "2021-09-03T23:13:45-04:00"
    },
    {
      "path": "index.html",
      "title": "Redistricting Analysis",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-09-03T23:13:45-04:00"
    },
    {
      "path": "methods.html",
      "title": "How this works",
      "description": "Some procedures an information common to all of our analyses.",
      "author": [],
      "contents": "\nRedistricting Simulation\nIn evaluating a redistricting plan, it is important to consider not just the partisan and other characteristics of the plan, but also what other plans might have been adopted in its stead. To do so, we generate a large number of random redistricting plans according to a set of criteria, using a recently-developed computer algorithm (McCartan and Imai 2020). This algorithm generates redistricting plans which:\nHave districts with equal populations (up to a chosen percentage deviation from full equality)\nHave geographically contiguous districts\nHave districts which are compact (i.e., no strange shapes)\nAre not generated using any political information.\nOften, we will also ensure that district boundaries follow county lines as much as possible.\nThe algorithm works by starting off with a blank map and drawing new districts one at a time, across many maps in parallel. After each round of drawing districts,\nOur Scoring System\nWe give each redistricting plan a Redistricting Analysis Plan Score, based on two criteria: proportionality and representativeness.\nProportionality\nThis is measured as the difference between the statewide Democratic popular vote and the share of seats won by Democrats. If negative, then the plan favors Democrats; if positive, it favors Republicans. An ideal plan would score zero. But often what scores are possible is an inevitable consequence of the state’s political geography. So we score a plan relative to the set of simulated, hypothetical plans. If it is in the top 20% of simulated plans by proportionality, it earns an “A”. If it is the next 20%, a “B”. And so on, with a plan in the bottom 20% scoring an “F”.\nRepresentativeness\nThis is measured as the odds than an average voter will be represented by a legislator of their preferred party; the higher this value is, the better. As with proportionality, how well a plan scores according to this metric depends on the state’s political geography, so we score a plan relative to the simulated plans. Just like with proportionality, if it is in the top 20% of hypothetical plans by proportionality, it earns an “A”, and so on.\nData\nMost data used in these analyses is sourced from the ALARM Project’s precinct-level data files, which use data from the 2020 Census and the Voting and Election Science Team. Cleaned data in formatted to work with our analysis software packages is available here.\nTo produce election data using 2020 precinct boundaries, election results were projected down to the 2010 block level using voting-age population as weights. Results for 2020 blocks were then estimated using 2010 blocks and the land-use-based crosswalk files from VEST. Finally, 2020 blocks were aggregated to 2020 precincts using the Census’ 2020 block assignment files.\n\n\n\n",
      "last_modified": "2021-09-03T23:13:46-04:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
